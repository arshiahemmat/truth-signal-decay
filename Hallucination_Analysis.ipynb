{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b79d5cf",
   "metadata": {},
   "source": [
    "# Hallucination as Signal Decay: Colab-Ready Walkthrough\n",
    "\n",
    "Welcome! This notebook accompanies our paper **\"When Models Know but Donâ€™t Say\"**. \n",
    "\n",
    "It is designed to be **completely standalone**. You can run this on Google Colab (using a T4 or A100 GPU) to verify our findings.\n",
    "\n",
    "### What we'll do:\n",
    "1.  **Reproduce the \"Knowledge Gap\"**: See the massive difference between what the model *knows* (Pairwise) and what it *says* (Generation).\n",
    "2.  **Trace the Signal**: Use the **Logit Lens** to watch the truth decay in real-time.\n",
    "3.  **Fix it**: Apply **Logit Mixing** to recover the correct answer.\n",
    "\n",
    "> **Note**: This notebook uses `meta-llama/Llama-3.1-8B`. You need access to this model on HuggingFace and a User Access Token.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6e441f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformer_lens circuitsvis datasets accelerate protobuf sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73e7a0e",
   "metadata": {},
   "source": [
    "## 0. Authentication\n",
    "Llama-3.1 is a gated model. Please provide your HuggingFace token below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b56f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d354e0",
   "metadata": {},
   "source": [
    "## 1. Setup & Load Model\n",
    "We use `TransformerLens` to hook into the model's internal activations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6719383",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "sns.set_context(\"notebook\", font_scale=1.2)\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "\n",
    "# Config\n",
    "MODEL_NAME = \"meta-llama/Llama-3.1-8B\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Loading {MODEL_NAME} on {DEVICE}...\")\n",
    "\n",
    "# Load model in 4-bit/8-bit if on Colab (optional, standard load here)\n",
    "# If you run out of memory on Colab T4, try:\n",
    "# model = HookedTransformer.from_pretrained(MODEL_NAME, device=DEVICE, load_in_8bit=True)\n",
    "\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device=DEVICE,\n",
    "    fold_ln=False,\n",
    "    center_writing_weights=False,\n",
    "    center_unembed=False\n",
    ")\n",
    "\n",
    "print(\"Model loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0541c2",
   "metadata": {},
   "source": [
    "## 2. The Knowledge Gap\n",
    "\n",
    "We examine **CounterFact** examples.\n",
    "*   **Pairwise**: Does the model rank True > False?\n",
    "*   **Generation**: Does the model output True?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bdc9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load a tiny subset for demo purposes\n",
    "dataset = load_dataset(\"NeelNanda/counterfact-tracing\", split=\"train[:10]\")\n",
    "\n",
    "print(f\"Checking {len(dataset)} examples...\")\n",
    "\n",
    "results = []\n",
    "\n",
    "for item in dataset:\n",
    "    prompt = item['prompt']\n",
    "    target_true = item['target_true'].strip()\n",
    "    target_false = item['target_false'].strip()\n",
    "    \n",
    "    # 1. Run Model\n",
    "    # prepend_bos=True is crucial for Llama\n",
    "    logits = model(prompt, prepend_bos=True)\n",
    "    last_token_logits = logits[0, -1, :]\n",
    "    \n",
    "    # 2. Check Generation (Top-1)\n",
    "    pred_token_id = torch.argmax(last_token_logits).item()\n",
    "    pred_token = model.to_string(pred_token_id)\n",
    "    is_correct_gen = (pred_token.strip().lower() == target_true.lower())\n",
    "    \n",
    "    # 3. Check Pairwise (True vs False)\n",
    "    # We need to prepend a space to match Llama tokenization usually\n",
    "    true_token_id = model.to_single_token(f\" {target_true}\")\n",
    "    false_token_id = model.to_single_token(f\" {target_false}\")\n",
    "    \n",
    "    true_logit = last_token_logits[true_token_id].item()\n",
    "    false_logit = last_token_logits[false_token_id].item()\n",
    "    is_correct_pair = (true_logit > false_logit)\n",
    "    \n",
    "    results.append({\n",
    "        \"Prompt\": prompt,\n",
    "        \"True\": target_true,\n",
    "        \"Pred\": pred_token,\n",
    "        \"Pairwise\": \"âœ…\" if is_correct_pair else \"âŒ\",\n",
    "        \"Gen\": \"âœ…\" if is_correct_gen else \"âŒ\"\n",
    "    })\n",
    "\n",
    "df_gap = pd.DataFrame(results)\n",
    "display(df_gap)\n",
    "\n",
    "# Filter for the phenomenon: Pairwise Correct but Generation Wrong\n",
    "gap = df_gap[df_gap['Pairwise'] == 'âœ…'][df_gap['Gen'] == 'âŒ']\n",
    "print(f\"\\nFound {len(gap)} examples where the model KNOWS but FAILS to generate.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be75890",
   "metadata": {},
   "source": [
    "## 3. Investigating \"Signal Decay\"\n",
    "\n",
    "Let's pick an error case and trace the probability of the correct token across all layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e19e9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(gap) > 0:\n",
    "    example = gap.iloc[0]\n",
    "else:\n",
    "    example = df_gap.iloc[0] # Fallback\n",
    "\n",
    "prompt = example['Prompt']\n",
    "target = example['True']\n",
    "\n",
    "print(f\"Analyzing Prompt: '{prompt}'\")\n",
    "print(f\"Target: '{target}'\")\n",
    "\n",
    "# Run with cache to get internals\n",
    "logits, cache = model.run_with_cache(prompt)\n",
    "target_id = model.to_single_token(f\" {target}\")\n",
    "\n",
    "layer_probs = []\n",
    "\n",
    "for layer in range(model.cfg.n_layers):\n",
    "    # Retrieve residual stream at the last token position\n",
    "    resid = cache[f\"blocks.{layer}.hook_resid_post\"][0, -1, :]\n",
    "    \n",
    "    # Apply LayerNorm and Unembedding (Logit Lens)\n",
    "    # Note: HookedTransformer makes this easy\n",
    "    normalized = model.ln_final(resid)\n",
    "    layer_logits = model.unembed(normalized)\n",
    "    \n",
    "    # Get Probability\n",
    "    probs = torch.softmax(layer_logits, dim=-1)\n",
    "    target_prob = probs[target_id].item()\n",
    "    layer_probs.append(target_prob)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(layer_probs, marker='o', linewidth=2.5, color='#d62728', label=f\"P({target})\")\n",
    "plt.title(f\"Signal Decay Trace: '{target}'\")\n",
    "plt.xlabel(\"Layer\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.axhline(0, color='gray', linestyle='--')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"Observation: The probability likely peaks around layer 15-20 and then drops.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccc2464",
   "metadata": {},
   "source": [
    "## 4. The Fix: Logit Mixing\n",
    "\n",
    "We identified that the signal exists in the middle. Let's use it!\n",
    "We will intervene by adding a fraction of the mid-layer logits to the final logits.\n",
    "\n",
    "$$ L_{final} \\leftarrow (1 - \\alpha) L_{final} + \\alpha L_{mid} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178c0db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.2\n",
    "mid_layer = 20\n",
    "\n",
    "# 1. Get Mid-Layer Logits\n",
    "resid_mid = cache[f\"blocks.{mid_layer}.hook_resid_post\"][0, -1, :]\n",
    "norm_mid = model.ln_final(resid_mid)\n",
    "logits_mid = model.unembed(norm_mid)\n",
    "\n",
    "# 2. Get Final Logits\n",
    "logits_final = logits[0, -1, :]\n",
    "\n",
    "# 3. Mix\n",
    "logits_mixed = (1 - alpha) * logits_final + (alpha) * logits_mid\n",
    "\n",
    "# 4. Check Result\n",
    "new_pred_id = torch.argmax(logits_mixed).item()\n",
    "new_pred = model.to_string(new_pred_id)\n",
    "\n",
    "print(f\"Original Prediction: '{example['Pred']}'\")\n",
    "print(f\"Intervention (alpha={alpha}): '{new_pred}'\")\n",
    "\n",
    "if new_pred.strip().lower() == target.lower():\n",
    "    print(\"ðŸš€ SUCCESS: Knowledge recovered!\")\n",
    "else:\n",
    "    print(\"Failed. Try tuning alpha or layer.\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
