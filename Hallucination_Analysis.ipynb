{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b45c8bdb",
   "metadata": {},
   "source": [
    "# Hallucination as Signal Decay: Interactive Walkthrough\n",
    "\n",
    "Welcome! This notebook accompanies our paper **\"When Models Know but Donâ€™t Say\"**. \n",
    "\n",
    "We're going to explore why LLMs often fail to produce facts they actually \"know\" (based on their internal states) and demonstrate how to recover that knowledge using **Logit Mixing**.\n",
    "\n",
    "### What we'll do:\n",
    "1.  **Reproduce the \"Knowledge Gap\"**: See the massive difference between what the model *knows* (Pairwise) and what it *says* (Generation).\n",
    "2.  **Trace the Signal**: Use the **Logit Lens** to watch the truth decay in real-time across layers.\n",
    "3.  **Fix it**: Apply our lightweight intervention to rescue the correct answer.\n",
    "\n",
    "> **Note**: This notebook uses `meta-llama/Llama-3.1-8B` by default. You'll need a GPU with ~24GB VRAM (or use 4-bit quantization).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5078e073",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "# Add src to path so we can import our modules\n",
    "sys.path.append(os.path.abspath(\"src\"))\n",
    "\n",
    "# Set up plotting style\n",
    "sns.set_context(\"notebook\", font_scale=1.2)\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "\n",
    "print(\"Environment set up. Ready to load models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a3b5c8",
   "metadata": {},
   "source": [
    "## 1. Load the Model\n",
    "\n",
    "We use `TransformerLens` for easy access to internal activations. \n",
    "*Tip: If you're low on memory, uncomment the `load_in_4bit` logic (though exact values might shift slightly).*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b956da06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "MODEL_NAME = \"meta-llama/Llama-3.1-8B\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Loading {MODEL_NAME} on {DEVICE}...\")\n",
    "\n",
    "# Load model (Standard FP16 for reproduction)\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device=DEVICE,\n",
    "    fold_ln=False,\n",
    "    center_writing_weights=False,\n",
    "    center_unembed=False\n",
    ")\n",
    "\n",
    "print(\"Model loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e000d0",
   "metadata": {},
   "source": [
    "## 2. The Knowledge Gap\n",
    "\n",
    "Let's look at a few examples from the **CounterFact** dataset. We'll check two things:\n",
    "1.  **Pairwise Accuracy**: Does the model assign higher probability to the *True* object than the *False* one? (i.e., Does it know?)\n",
    "2.  **Generation**: If we just let it generate, does it output the True object?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984f6f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from src.utils.eval_utils import get_answer_logits\n",
    "\n",
    "# Load a tiny subset for demo purposes\n",
    "dataset = load_dataset(\"NeelNanda/counterfact-tracing\", split=\"train[:10]\")\n",
    "\n",
    "print(f\"Checking {len(dataset)} examples...\")\n",
    "\n",
    "results = []\n",
    "for item in dataset:\n",
    "    prompt = item['prompt']\n",
    "    target_true = item['target_true'].strip()\n",
    "    target_false = item['target_false'].strip()\n",
    "    \n",
    "    # Run model\n",
    "    logits = model(prompt, prepend_bos=True)\n",
    "    last_token_logits = logits[0, -1, :]\n",
    "    \n",
    "    # Check Gen\n",
    "    pred_token = model.to_string(torch.argmax(last_token_logits))\n",
    "    is_correct_gen = (pred_token.strip().lower() == target_true.lower())\n",
    "    \n",
    "    # Check Pairwise\n",
    "    true_logit = last_token_logits[model.to_single_token(f\" {target_true}\")]\n",
    "    false_logit = last_token_logits[model.to_single_token(f\" {target_false}\")]\n",
    "    is_correct_pair = (true_logit > false_logit).item()\n",
    "    \n",
    "    results.append({\n",
    "        \"Prompt\": prompt,\n",
    "        \"True\": target_true,\n",
    "        \"Pred\": pred_token,\n",
    "        \"Pairwise\": \"âœ…\" if is_correct_pair else \"âŒ\",\n",
    "        \"Gen\": \"âœ…\" if is_correct_gen else \"âŒ\"\n",
    "    })\n",
    "\n",
    "df_gap = pd.DataFrame(results)\n",
    "display(df_gap)\n",
    "\n",
    "gap = df_gap[df_gap['Pairwise'] == 'âœ…'][df_gap['Gen'] == 'âŒ']\n",
    "print(f\"\\nFound {len(gap)} examples where the model KNOWS but FAILS to generate.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53284d1f",
   "metadata": {},
   "source": [
    "## 3. Investigating the \"Signal Decay\"\n",
    "\n",
    "Take one of those failed examples (where Pairwise is âœ… but Gen is âŒ). What happened inside the model? \n",
    "\n",
    "We'll use a **Logit Lens** trace. We decode the residual stream at every layer into the vocabulary.\n",
    "*   **Hypothesis**: The model *had* the correct token in the middle layers, but lost it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5585c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the first \"Overwritten\" error\n",
    "if len(gap) > 0:\n",
    "    example = gap.iloc[0]\n",
    "else:\n",
    "    # Fallback if 10 examples didn't catch one (unlikely, usually ~40% rate)\n",
    "    example = df_gap.iloc[0] \n",
    "\n",
    "prompt = example['Prompt']\n",
    "target = example['True']\n",
    "\n",
    "print(f\"Analyzing Prompt: '{prompt}'\")\n",
    "print(f\"Target: '{target}'\")\n",
    "\n",
    "# Run cache\n",
    "logits, cache = model.run_with_cache(prompt)\n",
    "target_id = model.to_single_token(f\" {target}\")\n",
    "\n",
    "layer_probs = []\n",
    "\n",
    "for layer in range(model.cfg.n_layers):\n",
    "    # Decode residual stream at this layer\n",
    "    resid = cache[f\"blocks.{layer}.hook_resid_post\"][0, -1, :]\n",
    "    normalized = model.ln_final(resid)\n",
    "    layer_logits = model.unembed(normalized)\n",
    "    \n",
    "    # Get prob of target\n",
    "    probs = torch.softmax(layer_logits, dim=-1)\n",
    "    target_prob = probs[target_id].item()\n",
    "    layer_probs.append(target_prob)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(layer_probs, marker='o', linewidth=2.5, color='#d62728')\n",
    "plt.title(f\"Trace of Token: '{target}'\")\n",
    "plt.xlabel(\"Layer\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.axhline(0, color='gray', linestyle='--')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"Look at that trajectory! Often you see a peak around Layer 20, followed by a collapse.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bacc971",
   "metadata": {},
   "source": [
    "## 4. The Fix: Logit Mixing\n",
    "\n",
    "Now that we know the \"truth\" is strong in the mid-layers, let's just helpful nudge the final output.\n",
    "\n",
    "We define a mixing intervention:\n",
    "$$ L_{final} = (1 - \\alpha) \\cdot L_{final} + \\alpha \\cdot L_{mid} $$\n",
    "\n",
    "Let's try $\\alpha=0.2$ (mixing in 20% of Layer 20).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef76360",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.2\n",
    "mid_layer = 20\n",
    "\n",
    "# Get mid logits (re-using cache)\n",
    "resid_mid = cache[f\"blocks.{mid_layer}.hook_resid_post\"][0, -1, :]\n",
    "norm_mid = model.ln_final(resid_mid)\n",
    "logits_mid = model.unembed(norm_mid)\n",
    "\n",
    "# Get final logits\n",
    "logits_final = logits[0, -1, :]\n",
    "\n",
    "# Mix\n",
    "logits_mixed = (1 - alpha) * logits_final + (alpha) * logits_mid\n",
    "\n",
    "# Check result\n",
    "new_pred_id = torch.argmax(logits_mixed)\n",
    "new_pred = model.to_string(new_pred_id)\n",
    "\n",
    "print(f\"Original Gen: '{example['Pred']}'\")\n",
    "print(f\"Intervention (alpha={alpha}): '{new_pred}'\")\n",
    "\n",
    "if new_pred.strip().lower() == target.lower():\n",
    "    print(\"ðŸš€ SUCCESS: Knowledge recovered!\")\n",
    "else:\n",
    "    print(\"No change. Maybe try a higher alpha?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f17066",
   "metadata": {},
   "source": [
    "## Wrap Up\n",
    "\n",
    "This was a simplified demo of the core finding: **Signal Decay**.\n",
    "\n",
    "The full analysis in the paper (and the `src/` folder) scales this up to thousands of examples, trains a dedicated probe to detect *when* to intervene (so we don't break correct answers), and evaluates OOD robustness.\n",
    "\n",
    "Feel free to play around with the `alpha` or try different prompts!\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
